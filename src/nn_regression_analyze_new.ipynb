{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week_day_names = {\n",
    "    \"Monday\": 1, \n",
    "    \"Tuesday\": 2, \n",
    "    \"Wednesday\": 3, \n",
    "    \"Thursday\": 4, \n",
    "    \"Friday\": 5, \n",
    "    \"Saturday\": 6, \n",
    "    \"Sunday\": 7\n",
    "}\n",
    "source_columns = {\n",
    "    'location':'source',\n",
    "    'temp':'source_temp',\n",
    "    'clouds': 'source_clouds',\n",
    "    'pressure':'source_pressure',\n",
    "    'rain': 'source_rain',\n",
    "    'humidity':'source_humidity',\n",
    "    'wind':'source_wind'\n",
    "}\n",
    "destination_columns = {\n",
    "    'location':'destination',\n",
    "    'temp':'destination_temp',\n",
    "    'clouds': 'destination_clouds',\n",
    "    'pressure':'destination_pressure',\n",
    "    'rain': 'destination_rain',\n",
    "    'humidity':'destination_humidity',\n",
    "    'wind':'destination_wind'\n",
    "}\n",
    "cab_type_dict = {\n",
    "    'Lyft': 0,\n",
    "    'Uber': 1\n",
    "}\n",
    "names_list = [\n",
    "    'distance','cab_type','time_stamp',\\\n",
    "    'destination', 'source', 'price', \\\n",
    "    'surge_multiplier', 'product_id', \\\n",
    "    'name', 'source_temp', 'source_location', \\\n",
    "    'source_clouds', 'source_pressure', \\\n",
    "    'source_rain', 'source_time_stamp', \\\n",
    "    'source_humidity', 'source_wind', \\\n",
    "    'destination_temp', 'destination_location', \\\n",
    "    'destination_clouds', 'destination_pressure', \\\n",
    "    'destination_rain', 'destination_time_stamp', \\\n",
    "    'destination_humidity', 'destination_wind'\n",
    "]\n",
    "\n",
    "inf = 10000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_heatmap(train, name = \"\"):\n",
    "    colormap = plt.cm.viridis\n",
    "    plt.figure(figsize=(24,24))\n",
    "    plt.title(\"Pearson Correlation of Features \" + name, y=1.05, size=15)\n",
    "    sns.heatmap(train.astype(float).corr(),linewidths=0.01,vmax=1.0,\n",
    "    square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(base):\n",
    "    return_base = []\n",
    "    for b in base:\n",
    "        return_base.append(int(b[1:]))\n",
    "    return return_base\n",
    "\n",
    "def analyze_dataframe_basic(filename, sourcepath = \"..\\\\data\\\\data_refactored\\\\\", n=2):\n",
    "    \"\"\"Odczyt danych z pliku, podstawowa analiza PCA\"\"\"\n",
    "    filepath = sourcepath+filename\n",
    "    train = pd.read_csv(filepath)\n",
    "    train = train.drop('Date/Time', axis=1)\n",
    "    train = train.drop('Year', axis=1)\n",
    "    train = train.replace({\"Day_Name\": week_day_names})\n",
    "    train['Base'] = convert(train['Base'])\n",
    "    print()\n",
    "    print(train.head())\n",
    "    draw_heatmap(train, filename) \n",
    "    \n",
    "def analyze_dataframe_radial(filename, sourcepath = \"..\\\\data\\\\data_refactored\\\\\", n=2):\n",
    "    \"\"\"Odczyt danych z pliku, podstawowa analiza PCA\"\"\"\n",
    "    filepath = sourcepath+filename\n",
    "    train = pd.read_csv(filepath)\n",
    "    train = train.drop('Date/Time', axis=1)\n",
    "    train = train.drop('Year', axis=1)\n",
    "    lat_mean, lon_mean = np.mean(train['Lat']), np.mean(train['Lon'])\n",
    "    train['Radius'] = np.sqrt((train['Lat'] - lat_mean) ** 2 + (train['Lon'] - lon_mean) ** 2)\n",
    "    train['Sin'] = train['Lon'] / train['Radius']\n",
    "    train['Cos'] = train['Lat'] / train['Radius']\n",
    "    train = train.drop('Lon', axis=1)\n",
    "    train = train.drop('Lat', axis=1)\n",
    "    train = train.replace({\"Day_Name\": week_day_names})\n",
    "    train['Base'] = convert(train['Base'])\n",
    "    train['Base'] -= min(train['Base'])\n",
    "    print()\n",
    "    print(train.head())\n",
    "    draw_heatmap(train, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uber-raw-data-14.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze_dataframe_basic(\"uber-raw-data-14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze_dataframe_radial(\"uber-raw-data-14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cab Rides Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dataframes(path=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"Function to join data from two files: cab_rides.csv, weather.xls\"\"\"\n",
    "    filename_1, filename_2 = \"cab_rides.csv\", \"weather.xls\"\n",
    "    prices_dataframe = pd.read_csv(path + filename_1).dropna(axis=0).reset_index(drop=True)\n",
    "    prices_dataframe = prices_dataframe.drop('id', axis=1)\n",
    "    weather_dataframe = pd.read_csv(path + filename_2).fillna(0).drop('time_stamp', axis=1)\n",
    "    weather_dataframe = weather_dataframe.groupby('location').mean().reset_index(drop=False)\n",
    "    src_weather_dataframe = weather_dataframe.rename(columns=source_columns)\n",
    "    des_weather_dataframe = weather_dataframe.rename(columns=destination_columns)\n",
    "    data = prices_dataframe.merge(src_weather_dataframe, on='source').merge(des_weather_dataframe, on='destination')\n",
    "    return data\n",
    "\n",
    "def onehot_encode(data, column, prefix):\n",
    "    \"\"\"Change singular int value into a list of 0s and 1 by onehot encode\"\"\"\n",
    "    onehot_columns = pd.get_dummies(data[column], prefix=prefix)\n",
    "    data = data.drop(column, axis=1)\n",
    "    data = pd.concat([data, onehot_columns], axis=1)    \n",
    "    return data\n",
    "\n",
    "def refactor_data(data):\n",
    "    \"\"\"Function to refactor and prepare data for further actions\"\"\"\n",
    "    data['cab_type'] = data['cab_type'].replace(cab_type_dict)\n",
    "    data = onehot_encode(data, column='destination', prefix='destination')\n",
    "    data = onehot_encode(data, column='source', prefix='source')\n",
    "    data = onehot_encode(data, column='product_id', prefix='product_id')\n",
    "    data = onehot_encode(data, column='name', prefix='name')\n",
    "    return data    \n",
    "\n",
    "def analyze_dataframe_basic(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    draw_heatmap(data, \"cab_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze_dataframe_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - predicting price of cab ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class LearningCurvesPlotter(keras.callbacks.Callback):\n",
    "    \"\"\"Callback to plot the learning curves for the model during training.\"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), self.metrics[metric], label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), self.metrics['val_' + metric], label='val_' + metric)\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks_list = [LearningCurvesPlotter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predition_score(prediction, actual_price):\n",
    "    \"\"\"Score model predictions - square average of the prediction error\"\"\"\n",
    "    n = actual_price.shape[0]\n",
    "    return np.sqrt(np.sum((np.array(prediction)-np.array(actual_price))**2)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataframe(data):\n",
    "    \"\"\"Split dataframe into feature data and labels - price values\"\"\"\n",
    "    y = data['price']\n",
    "    X = data.drop('price', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "def down_scale_vector(X_train, X_test, n):\n",
    "    \"\"\"Transform data - reduce dimentionality by using PCA\"\"\"\n",
    "    pca = PCA(n_components=n).fit(X_train)\n",
    "    X_train, X_test = pca.transform(X_train), pca.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_data(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  \n",
    "\n",
    "def score_model(model, X, y, n=2, m=50):\n",
    "    \"\"\"Score the model base on its performance on test subset\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    pred = pred.reshape(pred.shape[0])\n",
    "    return predition_score(pred, y)\n",
    "\n",
    "def error_analyze(model, X, y, n=2):\n",
    "    \"\"\"Analyze the influence of actual price on method error\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    prediction = pred.reshape(pred.shape[0])\n",
    "    n = y.shape[0]\n",
    "    minimal_price, maximal_price = int(np.min(y)), int(np.max(y))\n",
    "    error = np.abs(np.array(prediction)-np.array(y))\n",
    "    average_error, ns = [0] * (maximal_price - minimal_price + 1), [0] * (maximal_price - minimal_price + 1)\n",
    "    \n",
    "    for err, p, act_price in zip(error, prediction, y):\n",
    "        idx = int(act_price) - minimal_price\n",
    "        average_error[idx] = average_error[idx] * ns[idx] / (ns[idx] + 1) + err / (ns[idx] + 1)\n",
    "        ns[idx] += 1\n",
    "        if idx > 0:\n",
    "            average_error[idx - 1] = average_error[idx - 1] * ns[idx - 1] / (ns[idx - 1] + 1) + err / (ns[idx - 1] + 1)\n",
    "            ns[idx - 1] += 1 \n",
    "        if idx < len(average_error) - 1:\n",
    "            average_error[idx + 1] = average_error[idx + 1] * ns[idx + 1] / (ns[idx + 1] + 1) + err / (ns[idx + 1] + 1)\n",
    "            ns[idx + 1] += 1\n",
    "            \n",
    "    std = [0] * (maximal_price - minimal_price + 1)\n",
    "    for err, act_price in zip(error, y):\n",
    "        idx = int(act_price) - minimal_price\n",
    "        std[idx] += (err - average_error[idx]) ** 2 / ns[idx]\n",
    "        if idx > 0:\n",
    "            std[idx-1] += (err - average_error[idx-1]) ** 2 / ns[idx-1]\n",
    "        if idx < len(average_error) - 1:\n",
    "            std[idx+1] += (err - average_error[idx+1]) ** 2 / ns[idx+1]\n",
    "            \n",
    "    std = np.sqrt(np.array(std))\n",
    "            \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Distribution of actual prices in test subset\")\n",
    "    plt.xlabel(\"Actual price [$]\")\n",
    "    plt.ylabel(\"Number of cab rides with fixed price\")\n",
    "    counts, bins = np.histogram(y, bins = (maximal_price - minimal_price) // 3)\n",
    "    plt.hist(bins[:-1], bins, weights=counts)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Distribution of predicted prices in test subset\")\n",
    "    plt.xlabel(\"Predicted price [$]\")\n",
    "    plt.ylabel(\"Number of cab rides with prediced price\")\n",
    "    counts, bins = np.histogram(prediction, bins = (maximal_price - minimal_price) // 3)\n",
    "    plt.hist(bins[:-1], bins, weights=counts)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Prediction error\")\n",
    "    plt.xlabel(\"Actual price [$]\")\n",
    "    plt.ylabel(\"Average error [$]\")\n",
    "    plt.grid()\n",
    "    plt.errorbar([i for i in range(minimal_price, maximal_price + 1)], average_error, np.sqrt(std), linestyle='None', marker='.')\n",
    "    plt.show()\n",
    "    \n",
    "def draw_result_3d(model, X, y, n=2, m=50):\n",
    "    \"\"\"Draw predictions and actual prices in 3d plot and compare them\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    pred = pred.reshape(pred.shape[0])\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=X[0:m,0], y=X[0:m,1], z=y[0:m], mode='markers', name=\"Actual price\"))\n",
    "    fig.add_trace(go.Scatter3d(x=X[0:m,0], y=X[0:m,1], z=pred[0:m], mode='markers', name=\"Predicted price\"))\n",
    "    fig.show()\n",
    "    print(\"Prediction score square sum =\", predition_score(pred, y))\n",
    "    for i in range(m): print(\"actual price =\", \"{:5.2f}\".format(y[i]), \"predicted price =\", \\\n",
    "                        \"{:5.2f}\".format(abs(pred[i])))\n",
    "    \n",
    "def linear_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, m=50, \\\n",
    "                            do_print_ceof=False, do_draw_results=True, \\\n",
    "                            do_score=False, do_analyze_error = False):\n",
    "    \"\"\"Linear regression model - predict prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.coef_, model.intercept_)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.score(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "\n",
    "def gaussian_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                              m=50, do_print_ceof=False, do_draw_results=True, \\\n",
    "                              do_score=False, do_analyze_error = False):\n",
    "    \"\"\"Gaussian regression model - predict prices\"\"\"\n",
    "    \"\"\"Giving some a priori weights is probably necessary\"\"\"\n",
    "    \"\"\"and may have practical usage in our project - uber\"\"\"\n",
    "    \"\"\"may want to give some general prediction about prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    size = min(X_train.shape[0], size)\n",
    "    X_train, X_test, y_train, y_test = X_train[:size,:], X_test[:size // 4,:], y_train[:size], y_test[:size // 4]\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.L_, model.alpha_)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.score(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "        \n",
    "def nn_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, m=50, \\\n",
    "                        do_print_ceof=False, do_draw_results=True, epochs=10, \\\n",
    "                        do_score=False, layer_n=1, do_analyze_error=False, \\\n",
    "                        do_analyze_epochs=False):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    layer_width = 8\n",
    "    if n: layer_width = int(np.sqrt(n))\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    if do_analyze_epochs:  \n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = epochs, verbose=1, callbacks=callbacks_list)\n",
    "    else:  \n",
    "        model.fit(X_train, y_train, epochs=epochs)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.evaluate(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_dimensionality_lin_reg():\n",
    "    ns, scores = [65], []\n",
    "    best_n, best_score = 65, linear_regression_model(do_draw_results = False, do_score = True)\n",
    "    scores.append(best_score)\n",
    "    for n in range(64, 2, -2):\n",
    "        score = linear_regression_model(n = n, do_draw_results = False, do_score = True)\n",
    "        if score < best_score: best_n, best_score = n, score\n",
    "        ns.append(n)\n",
    "        scores.append(score)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.title(\"Linear regression score for different data dimensionality\")\n",
    "    plt.xlabel(\"n - data dimensionality\")\n",
    "    plt.ylabel(\"Linear regression score\")\n",
    "    plt.plot(ns, scores)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}\")\n",
    "    \n",
    "    return best_n\n",
    "    \n",
    "def find_best_nn():\n",
    "    ns, epochs, layers_ns, scores = [], [], [], []\n",
    "    best_score, best_n, best_epoch, best_layer_n = inf, 0, 0, 0\n",
    "    for n in [10, 30, 50, False]:\n",
    "        epoch = 10\n",
    "        for layer_n in range(3):            \n",
    "            print(f\"n = {n}, epoch = {epoch}, layer_n = {layer_n}\")\n",
    "            score = nn_regression_model(n = n, epochs = epoch, do_draw_results = False, \\\n",
    "                                        do_score = True, layer_n = layer_n)\n",
    "            if score < best_score: \n",
    "                best_n, best_score, best_epoch, best_layer_n = n, score, epoch, layer_n\n",
    "            ns.append(n)\n",
    "            layers_ns.append(layer_n)\n",
    "            scores.append(score)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=ns, y=layers_ns, z=scores, mode='markers', name=\"Prediction score\"))\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}, epoch = {best_epoch}, layer number = {best_layer_n + 1}\")\n",
    "    \n",
    "    return best_n, best_epoch, best_layer_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_n, best_epoch, best_layer_n = 50, 10, 1\n",
    "best_n, best_epoch, best_layer_n = find_best_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_n = 50\n",
    "best_layer_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = best_epoch, layer_n = best_layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = best_epoch, layer_n = best_layer_n, do_draw_results=False, do_analyze_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = 50, layer_n = best_layer_n, do_analyze_epochs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cab rides analyze with exact condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_dict(weather_dataframe):\n",
    "    loc_dict = dict()\n",
    "    for loc in np.unique(weather_dataframe['location']): \n",
    "        loc_dict[loc] = []\n",
    "    return loc_dict\n",
    "\n",
    "def fill_dict(loc_dict, weather_dataframe):\n",
    "    for row in weather_dataframe.to_numpy(): \n",
    "        loc_dict[row[1]].append(row)\n",
    "    return loc_dict\n",
    " \n",
    "def sort_cond(loc_dict):\n",
    "    for loc, rows in loc_dict.items(): \n",
    "        rows.sort(key=lambda x: x[5])\n",
    "    return loc_dict\n",
    "\n",
    "def find_condition(loc_dict, row, i):\n",
    "    t, conditions = row[2], loc_dict[row[i]]\n",
    "    a, b = 0, len(conditions) - 1\n",
    "    while a < b - 1:\n",
    "        c = (a + b) // 2\n",
    "        if conditions[c][5] < t: a = c\n",
    "        else: b = c\n",
    "    return conditions[c]    \n",
    "\n",
    "def append_rows(loc_dict, prices_dataframe):\n",
    "    data, data_merged = prices_dataframe.to_numpy(), []\n",
    "    for row in data:\n",
    "        row = np.append(row, find_condition(loc_dict, row, 4), 0)\n",
    "        data_merged.append(np.append(row, find_condition(loc_dict, row, 3), 0))\n",
    "    return data_merged\n",
    "\n",
    "def join_dataframes(path=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"Function to join data from two files: cab_rides.csv, weather.xls\"\"\"\n",
    "    filename_1, filename_2 = \"cab_rides.csv\", \"weather.xls\"\n",
    "    prices_dataframe = pd.read_csv(path + filename_1).dropna(axis=0).reset_index(drop=True)\n",
    "    prices_dataframe = prices_dataframe.drop('id', axis=1)\n",
    "    prices_dataframe['time_stamp'] = prices_dataframe['time_stamp'] // 1000\n",
    "    weather_dataframe = pd.read_csv(path + filename_2).fillna(0)\n",
    "    loc_dict = init_dict(weather_dataframe)\n",
    "    loc_dict = fill_dict(loc_dict, weather_dataframe)\n",
    "    loc_dict = sort_cond(loc_dict)\n",
    "    data_merged = append_rows(loc_dict, prices_dataframe)\n",
    "    df = pd.DataFrame(np.array(data_merged), columns = names_list)\n",
    "    df = df.drop('destination_location', axis=1).drop('source_location', axis=1)\n",
    "    df = df.drop('destination_time_stamp', axis=1).drop('source_time_stamp', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = join_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def analyze_dataframe_joined(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = join_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    draw_heatmap(data, \"cab_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, \\\n",
    "                                      do_print_ceof=False, do_draw_results=True, \\\n",
    "                                      do_score = False, do_analyze_error=False):\n",
    "    \"\"\"Linear regression model - predict prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.coef_, model.intercept_)\n",
    "    print(\"Test score =\", model.score(X_test, y_test))\n",
    "    if do_draw_results: draw_result_3d(model, X_test, y_test)  \n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "\n",
    "def gaussian_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                                        do_print_ceof=False, do_draw_results=True, \\\n",
    "                                        do_score = False, do_analyze_error=False):\n",
    "    \"\"\"Gaussian regression model - predict prices\"\"\"\n",
    "    \"\"\"Giving some a priori weights is probably necessary\"\"\"\n",
    "    \"\"\"and may have practical usage in our project - uber\"\"\"\n",
    "    \"\"\"may want to give some general prediction about prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    size = min(X_train.shape[0], size)\n",
    "    X_train, X_test, y_train, y_test = X_train[:size,:], X_test[:size // 4,:], y_train[:size], y_test[:size // 4]\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.L_, model.alpha_)\n",
    "    print(\"Test score =\", model.score(X_test, y_test))\n",
    "    if do_draw_results: draw_result_3d(model, X_test, y_test)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "        \n",
    "def nn_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                                  do_print_ceof=False, do_draw_results=True, epochs=10, \\\n",
    "                                  m=50, do_score=False, layer_n=1, do_analyze_error=False, \\\n",
    "                                  do_analyze_epochs=False):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    X_train, X_test, y_train, y_test = X_train.astype('float32'), X_test.astype('float32'), y_train.astype('float32'), y_test.astype('float32')\n",
    "    layer_width = 8\n",
    "    if n: layer_width = int(np.sqrt(n))\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(layer_width, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    if do_analyze_epochs:  \n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = epochs, verbose=1, callbacks=callbacks_list)\n",
    "    else:  \n",
    "        model.fit(X_train, y_train, epochs=epochs)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.evaluate(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyze_dataframe_joined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_dimensionality_lin_reg_condition():\n",
    "    ns, scores = [65], []\n",
    "    best_n, best_score = 65, linear_regression_model_condition(do_draw_results = False, do_score = True)\n",
    "    scores.append(best_score)\n",
    "    for n in range(64, 2, -2):\n",
    "        score = linear_regression_model_condition(n = n, do_draw_results = False, do_score = True)\n",
    "        if score < best_score: best_n, best_score = n, score\n",
    "        ns.append(n)\n",
    "        scores.append(score)\n",
    "    plt.title(\"Linear regression score for different data dimensionality\")\n",
    "    plt.xlabel(\"n - data dimensionality\")\n",
    "    plt.ylabel(\"Linear regression score\")\n",
    "    plt.plot(ns, scores)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}\")\n",
    "    \n",
    "    return best_n\n",
    "    \n",
    "def find_best_nn_condition():\n",
    "    ns, epochs, layers_ns, scores = [], [], [], []\n",
    "    best_score, best_n, best_epoch, best_layer_n = inf, 0, 0, 0\n",
    "    for n in [10, 30, 50, False]:\n",
    "        epoch = 10\n",
    "        for layer_n in range(3):            \n",
    "            print(f\"n = {n}, epoch = {epoch}, layer_n = {layer_n}\")\n",
    "            score = nn_regression_model_condition(n = n, epochs = epoch, do_draw_results = False, \\\n",
    "                                        do_score = True, layer_n = layer_n)\n",
    "            if score < best_score: \n",
    "                best_n, best_score, best_epoch, best_layer_n = n, score, epoch, layer_n\n",
    "            ns.append(n)\n",
    "            layers_ns.append(layer_n)\n",
    "            scores.append(score)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=ns, y=layers_ns, z=scores, mode='markers', name=\"Prediction score\"))\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}, epoch = {best_epoch}, layer number = {best_layer_n + 1}\")\n",
    "    \n",
    "    return best_n, best_epoch, best_layer_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best_n, best_epoch, best_layer_n = 50, 10, 1\n",
    "best_n, best_epoch, best_layer_n = find_best_nn_condition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_regression_model_condition(n = best_n, epochs = best_epoch, layer_n = best_layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_regression_model_condition(n = best_n, epochs = best_epoch, layer_n = best_layer_n, do_draw_results=False, do_analyze_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model_condition(n = best_n, epochs = 50, layer_n = best_layer_n, do_draw_results=False, do_analyze_epochs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    return data.columns, data\n",
    "\n",
    "def split_dataframe_analyse(data):\n",
    "    \"\"\"Split dataframe into feature data and labels - price values\"\"\"\n",
    "    y = data['price']\n",
    "    X = data.drop('price', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "def prepare_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def prepare_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  \n",
    "\n",
    "def compare_scale(X_1, X_2):\n",
    "    r = [1] * 64\n",
    "    for i in range(64):\n",
    "        r[i] = (np.max(X_2[:,i]) - np.min(X_2[:,i])) / (np.max(X_1[:,i]) - np.min(X_1[:,i]))\n",
    "    r[2] = 1\n",
    "    return r\n",
    "\n",
    "def nn_regression_model(X_train, X_test, y_train, y_test, epochs=10, layer_n=1):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    layer_width = 8\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def average_features(X):\n",
    "    idx_av = [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    group1 = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "    group2 = [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
    "    group3 = [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
    "    group4 = [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
    "    idx_moda = [1]\n",
    "    x = [0] * 64\n",
    "    for i in range(64): x[i] = np.min(X[:,i])\n",
    "    m1, m2 = [0] * 64, [0] * 64\n",
    "    for i in idx_av: x[i] = np.mean(X[:,i])\n",
    "    y = [0] * 64\n",
    "    for i in group1: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group2: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group3: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group4: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    for i in range(64):\n",
    "        m1[i] = np.min(X[:,i])\n",
    "        m2[i] = np.max(X[:,i])\n",
    "    for i in idx_moda:\n",
    "        values, counts = np.unique(X[i], return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        x[i] = values[ind]\n",
    "    return x, m1, m2\n",
    "    \n",
    "# model = nn_regression_model(X_train, X_test, y_train, y_test, epochs = 50, layer_n = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_analyse(\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "x_average, min_x, max_x = average_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = nn_regression_model(X_train, X_test, y_train, y_test, epochs = 10, layer_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coeff(model, x_average, min_x, max_x, i):\n",
    "    xs, ys = [], []\n",
    "    x_av = x_average.copy()\n",
    "    for j in range(11):\n",
    "        x = min_x[i] + (max_x[i] - min_x[i]) * j / 10\n",
    "        x_av[i] = x \n",
    "        xs.append(x)\n",
    "        ys.append(model.predict(np.array(x_av).reshape(1, -1)))\n",
    "    Reg = LinearRegression().fit(np.array(xs).reshape(-1, 1), np.array(ys).reshape(-1, 1))\n",
    "    return Reg.coef_[0][0]\n",
    "\n",
    "def find_coeff_2(model, x_average, min_x, max_x, i, a):\n",
    "    xs, ys = [], []\n",
    "    x_av = x_average.copy()\n",
    "    x_av[a] = min_x[a]\n",
    "    for j in range(2):\n",
    "        x = (1 - j) * min_x[i] + max_x[i] * j\n",
    "        x_av[i] = x \n",
    "        xs.append(x)\n",
    "        ys.append(model.predict(np.array(x_av).reshape(1, -1)))\n",
    "    return -((ys[1] - ys[0]) / (xs[1] - ys[0]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0] * 64\n",
    "for i in [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    a[i] = find_coeff(model, x_average, min_x, max_x, i)\n",
    "for i in [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 21)\n",
    "for i in [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 29)\n",
    "for i in [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 41)\n",
    "for i in [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 52)\n",
    "for i in [1]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = prepare_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "r = compare_scale(X_train_n, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_1 = 1.7281874396448482\n",
      "a_2 = -0.033172754170394995\n",
      "a_3 = -0.007703390319825178\n",
      "a_4 = 0.09429139654720513\n",
      "a_5 = 0.030214403270170307\n",
      "a_6 = -0.0009285430702586802\n",
      "a_7 = -0.001963079586083003\n",
      "a_8 = 0.0004459240233283986\n",
      "a_9 = -0.0005290000392996817\n",
      "a_10 = 0.01648250000476953\n",
      "a_11 = -0.04597528227748857\n",
      "a_12 = -0.00029385222745008615\n",
      "a_13 = -0.0020946716428867167\n",
      "a_14 = 9.540353749259363e-05\n",
      "a_15 = 0.002472382158232821\n",
      "a_16 = -0.017243990488392254\n",
      "a_17 = -0.034455406800808316\n",
      "a_18 = -0.022996514090624958\n",
      "a_19 = -0.009913084604313659\n",
      "a_20 = -0.047913116456126864\n",
      "a_21 = -0.003552630934770566\n",
      "a_22 = -0.0821946272579453\n",
      "a_23 = -0.0635762538232038\n",
      "a_24 = -0.03940554626524118\n",
      "a_25 = -0.05210438537100727\n",
      "a_26 = 0.044808645162271746\n",
      "a_27 = -0.03898264026096\n",
      "a_28 = -0.06288866707401108\n",
      "a_29 = -0.004180659385036129\n",
      "a_30 = 0.1922646885916937\n",
      "a_31 = -0.06243164304130616\n",
      "a_32 = -0.053998139874424665\n",
      "a_33 = -0.19119357128993814\n",
      "a_34 = -0.005371960146473856\n",
      "a_35 = 0.06545204569950223\n",
      "a_36 = 0.07096206132482838\n",
      "a_37 = -0.07635533690666539\n",
      "a_38 = 0.06529281428834459\n",
      "a_39 = -0.007526143829506273\n",
      "a_40 = 0.08953941997080457\n",
      "a_41 = -0.15734492558370314\n",
      "a_42 = 0.15356614528624407\n",
      "a_43 = 0.07830519822479894\n",
      "a_44 = -0.16961233974282003\n",
      "a_45 = 0.012575583976795465\n",
      "a_46 = -0.2242115797479283\n",
      "a_47 = -0.020186572868655963\n",
      "a_48 = -0.19450263140289553\n",
      "a_49 = 0.04259166548342095\n",
      "a_50 = 0.1572083211009801\n",
      "a_51 = -0.007019832835309161\n",
      "a_52 = 0.04080149799177657\n",
      "a_53 = 0.05745210177928199\n",
      "a_54 = 0.19929670886241915\n",
      "a_55 = 0.08681048894239406\n",
      "a_56 = 0.16640240501547374\n",
      "a_57 = 0.06120306388740033\n",
      "a_58 = -0.0734185959013586\n",
      "a_59 = 0.0019235412549692815\n",
      "a_60 = -0.17812766261960084\n",
      "a_61 = -0.04019012166769417\n",
      "a_62 = -0.15503452223618866\n",
      "a_63 = -0.10400180502985075\n",
      "a_64 = -0.18031141939456086\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "    print(f\"a_{i+1} = {a[i] / r[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def preprocess_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data_analyse(\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "x_average, min_x, max_x = average_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15950/15950 [==============================] - 24s 1ms/step - loss: 6.0285 - mae: 1.4221\n",
      "Epoch 2/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.2613 - mae: 1.2018\n",
      "Epoch 3/10\n",
      "15950/15950 [==============================] - 24s 1ms/step - loss: 3.2049 - mae: 1.1905\n",
      "Epoch 4/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.1826 - mae: 1.1842\n",
      "Epoch 5/10\n",
      "15950/15950 [==============================] - 23s 1ms/step - loss: 3.1531 - mae: 1.1780\n",
      "Epoch 6/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.1338 - mae: 1.1731\n",
      "Epoch 7/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.1097 - mae: 1.1685\n",
      "Epoch 8/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.1010 - mae: 1.1648\n",
      "Epoch 9/10\n",
      "15950/15950 [==============================] - 22s 1ms/step - loss: 3.0916 - mae: 1.1629\n",
      "Epoch 10/10\n",
      "15950/15950 [==============================] - 23s 1ms/step - loss: 3.0849 - mae: 1.1615\n"
     ]
    }
   ],
   "source": [
    "model = nn_regression_model(X_train, X_test, y_train, y_test, epochs = 10, layer_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0] * 64\n",
    "for i in [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    a[i] = find_coeff(model, x_average, min_x, max_x, i)\n",
    "for i in [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 21)\n",
    "for i in [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 29)\n",
    "for i in [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 41)\n",
    "for i in [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 52)\n",
    "for i in [1]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = prepare_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "r = compare_scale(X_train_n, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_1 = 1.8087729349292139\n",
      "a_2 = 0.015453435518368521\n",
      "a_3 = -0.007023804226533108\n",
      "a_4 = 0.08060397985445947\n",
      "a_5 = -0.0022257800822313833\n",
      "a_6 = 4.511971505618167e-05\n",
      "a_7 = -0.001392311522880054\n",
      "a_8 = 6.40487889008379e-05\n",
      "a_9 = 8.06184989962624e-05\n",
      "a_10 = 0.022354052986832962\n",
      "a_11 = -0.004822666095025653\n",
      "a_12 = 0.0002771053974709536\n",
      "a_13 = 0.0003298762142541236\n",
      "a_14 = -0.00014990485911193274\n",
      "a_15 = 0.0011652005553681832\n",
      "a_16 = -0.0034950885490315675\n",
      "a_17 = -0.00022469388734488194\n",
      "a_18 = 0.013197155834521742\n",
      "a_19 = 0.10049621465080079\n",
      "a_20 = 0.02763551966489167\n",
      "a_21 = -0.1173193009758732\n",
      "a_22 = 0.018704741260802328\n",
      "a_23 = 0.0475505999944598\n",
      "a_24 = 0.04278228068876329\n",
      "a_25 = 0.11441747341609197\n",
      "a_26 = 0.05635861890728711\n",
      "a_27 = -0.14962826557679193\n",
      "a_28 = 0.06132757880385109\n",
      "a_29 = -0.010332625578919218\n",
      "a_30 = 0.13594490025996758\n",
      "a_31 = -0.1992417851433546\n",
      "a_32 = -0.08987264077882126\n",
      "a_33 = 0.04182936077469115\n",
      "a_34 = -0.07461295592217608\n",
      "a_35 = -0.1577561469921145\n",
      "a_36 = 0.02717163386848169\n",
      "a_37 = 0.046717505972155134\n",
      "a_38 = 0.059141948265631815\n",
      "a_39 = 0.08701512239440724\n",
      "a_40 = -0.010170679317271237\n",
      "a_41 = 0.021028867241430993\n",
      "a_42 = -0.04907967707335159\n",
      "a_43 = 0.10138483612536081\n",
      "a_44 = -0.03303758692151764\n",
      "a_45 = -0.02266126925899132\n",
      "a_46 = 0.004233596923125568\n",
      "a_47 = -0.0861695445838485\n",
      "a_48 = -0.03862846955896231\n",
      "a_49 = 0.03854588962163881\n",
      "a_50 = 0.103096844746119\n",
      "a_51 = 0.06083499613430469\n",
      "a_52 = 0.0901700641612885\n",
      "a_53 = -0.0505911052395331\n",
      "a_54 = 0.06826799606538833\n",
      "a_55 = 0.0683584082234776\n",
      "a_56 = 0.06676815459287332\n",
      "a_57 = 0.23743850541873163\n",
      "a_58 = -0.03939271295108429\n",
      "a_59 = 0.07296375036201473\n",
      "a_60 = -0.1170187102824357\n",
      "a_61 = -0.022807845613307214\n",
      "a_62 = 0.008055677560601908\n",
      "a_63 = 0.022314063313155805\n",
      "a_64 = 0.027969352341517168\n"
     ]
    }
   ],
   "source": [
    "for i in range(64):\n",
    "    print(f\"a_{i+1} = {a[i] / r[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
