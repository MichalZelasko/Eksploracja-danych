{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.offline as pyo\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "pyo.init_notebook_mode()\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_day_names = {\n",
    "    \"Monday\": 1, \n",
    "    \"Tuesday\": 2, \n",
    "    \"Wednesday\": 3, \n",
    "    \"Thursday\": 4, \n",
    "    \"Friday\": 5, \n",
    "    \"Saturday\": 6, \n",
    "    \"Sunday\": 7\n",
    "}\n",
    "source_columns = {\n",
    "    'location':'source',\n",
    "    'temp':'source_temp',\n",
    "    'clouds': 'source_clouds',\n",
    "    'pressure':'source_pressure',\n",
    "    'rain': 'source_rain',\n",
    "    'humidity':'source_humidity',\n",
    "    'wind':'source_wind'\n",
    "}\n",
    "destination_columns = {\n",
    "    'location':'destination',\n",
    "    'temp':'destination_temp',\n",
    "    'clouds': 'destination_clouds',\n",
    "    'pressure':'destination_pressure',\n",
    "    'rain': 'destination_rain',\n",
    "    'humidity':'destination_humidity',\n",
    "    'wind':'destination_wind'\n",
    "}\n",
    "cab_type_dict = {\n",
    "    'Lyft': 0,\n",
    "    'Uber': 1\n",
    "}\n",
    "names_list = [\n",
    "    'distance','cab_type','time_stamp',\\\n",
    "    'destination', 'source', 'price', \\\n",
    "    'surge_multiplier', 'product_id', \\\n",
    "    'name', 'source_temp', 'source_location', \\\n",
    "    'source_clouds', 'source_pressure', \\\n",
    "    'source_rain', 'source_time_stamp', \\\n",
    "    'source_humidity', 'source_wind', \\\n",
    "    'destination_temp', 'destination_location', \\\n",
    "    'destination_clouds', 'destination_pressure', \\\n",
    "    'destination_rain', 'destination_time_stamp', \\\n",
    "    'destination_humidity', 'destination_wind'\n",
    "]\n",
    "\n",
    "inf = 10000000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_heatmap(train, name = \"\"):\n",
    "    colormap = plt.cm.viridis\n",
    "    plt.figure(figsize=(24,24))\n",
    "    plt.title(\"Pearson Correlation of Features \" + name, y=1.05, size=15)\n",
    "    sns.heatmap(train.astype(float).corr(),linewidths=0.01,vmax=1.0,\n",
    "    square=True, cmap=colormap, linecolor='white', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(base):\n",
    "    return_base = []\n",
    "    for b in base:\n",
    "        return_base.append(int(b[1:]))\n",
    "    return return_base\n",
    "\n",
    "def analyze_dataframe_basic(filename, sourcepath = \"..\\\\data\\\\data_refactored\\\\\", n=2):\n",
    "    \"\"\"Odczyt danych z pliku, podstawowa analiza PCA\"\"\"\n",
    "    filepath = sourcepath+filename\n",
    "    train = pd.read_csv(filepath)\n",
    "    train = train.drop('Date/Time', axis=1)\n",
    "    train = train.drop('Year', axis=1)\n",
    "    train = train.replace({\"Day_Name\": week_day_names})\n",
    "    train['Base'] = convert(train['Base'])\n",
    "    print()\n",
    "    print(train.head())\n",
    "    draw_heatmap(train, filename) \n",
    "    \n",
    "def analyze_dataframe_radial(filename, sourcepath = \"..\\\\data\\\\data_refactored\\\\\", n=2):\n",
    "    \"\"\"Odczyt danych z pliku, podstawowa analiza PCA\"\"\"\n",
    "    filepath = sourcepath+filename\n",
    "    train = pd.read_csv(filepath)\n",
    "    train = train.drop('Date/Time', axis=1)\n",
    "    train = train.drop('Year', axis=1)\n",
    "    lat_mean, lon_mean = np.mean(train['Lat']), np.mean(train['Lon'])\n",
    "    train['Radius'] = np.sqrt((train['Lat'] - lat_mean) ** 2 + (train['Lon'] - lon_mean) ** 2)\n",
    "    train['Sin'] = train['Lon'] / train['Radius']\n",
    "    train['Cos'] = train['Lat'] / train['Radius']\n",
    "    train = train.drop('Lon', axis=1)\n",
    "    train = train.drop('Lat', axis=1)\n",
    "    train = train.replace({\"Day_Name\": week_day_names})\n",
    "    train['Base'] = convert(train['Base'])\n",
    "    train['Base'] -= min(train['Base'])\n",
    "    print()\n",
    "    print(train.head())\n",
    "    draw_heatmap(train, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# uber-raw-data-14.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe_basic(\"uber-raw-data-14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe_radial(\"uber-raw-data-14.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cab Rides Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(path=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"Function to join data from two files: cab_rides.csv, weather.xls\"\"\"\n",
    "    filename_1, filename_2 = \"cab_rides.csv\", \"weather.xls\"\n",
    "    prices_dataframe = pd.read_csv(path + filename_1).dropna(axis=0).reset_index(drop=True)\n",
    "    prices_dataframe = prices_dataframe.drop('id', axis=1)\n",
    "    weather_dataframe = pd.read_csv(path + filename_2).fillna(0).drop('time_stamp', axis=1)\n",
    "    weather_dataframe = weather_dataframe.groupby('location').mean().reset_index(drop=False)\n",
    "    src_weather_dataframe = weather_dataframe.rename(columns=source_columns)\n",
    "    des_weather_dataframe = weather_dataframe.rename(columns=destination_columns)\n",
    "    data = prices_dataframe.merge(src_weather_dataframe, on='source').merge(des_weather_dataframe, on='destination')\n",
    "    return data\n",
    "\n",
    "def onehot_encode(data, column, prefix):\n",
    "    \"\"\"Change singular int value into a list of 0s and 1 by onehot encode\"\"\"\n",
    "    onehot_columns = pd.get_dummies(data[column], prefix=prefix)\n",
    "    data = data.drop(column, axis=1)\n",
    "    data = pd.concat([data, onehot_columns], axis=1)    \n",
    "    return data\n",
    "\n",
    "def refactor_data(data):\n",
    "    \"\"\"Function to refactor and prepare data for further actions\"\"\"\n",
    "    data['cab_type'] = data['cab_type'].replace(cab_type_dict)\n",
    "    data = onehot_encode(data, column='destination', prefix='destination')\n",
    "    data = onehot_encode(data, column='source', prefix='source')\n",
    "    data = onehot_encode(data, column='product_id', prefix='product_id')\n",
    "    data = onehot_encode(data, column='name', prefix='name')\n",
    "    return data    \n",
    "\n",
    "def analyze_dataframe_basic(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    draw_heatmap(data, \"cab_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models - predicting price of cab ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class LearningCurvesPlotter(keras.callbacks.Callback):\n",
    "    \"\"\"Callback to plot the learning curves for the model during training.\"\"\"\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "            \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "        \n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        f, axs = plt.subplots(1, len(metrics), figsize=(15,5))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            axs[i].plot(range(1, epoch + 2), self.metrics[metric], label=metric)\n",
    "            if logs['val_' + metric]:\n",
    "                axs[i].plot(range(1, epoch + 2), self.metrics['val_' + metric], label='val_' + metric)\n",
    "            axs[i].legend()\n",
    "            axs[i].grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [LearningCurvesPlotter()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predition_score(prediction, actual_price):\n",
    "    \"\"\"Score model predictions - square average of the prediction error\"\"\"\n",
    "    n = actual_price.shape[0]\n",
    "    return np.sqrt(np.sum((np.array(prediction)-np.array(actual_price))**2)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(data):\n",
    "    \"\"\"Split dataframe into feature data and labels - price values\"\"\"\n",
    "    y = data['price']\n",
    "    X = data.drop('price', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "    return X_train, X_test, y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "def down_scale_vector(X_train, X_test, n):\n",
    "    \"\"\"Transform data - reduce dimentionality by using PCA\"\"\"\n",
    "    pca = PCA(n_components=n).fit(X_train)\n",
    "    X_train, X_test = pca.transform(X_train), pca.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def prepare_data(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  \n",
    "\n",
    "def score_model(model, X, y, n=2, m=50):\n",
    "    \"\"\"Score the model base on its performance on test subset\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    pred = pred.reshape(pred.shape[0])\n",
    "    return predition_score(pred, y)\n",
    "\n",
    "def error_analyze(model, X, y, n=2):\n",
    "    \"\"\"Analyze the influence of actual price on method error\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    prediction = pred.reshape(pred.shape[0])\n",
    "    n = y.shape[0]\n",
    "    minimal_price, maximal_price = int(np.min(y)), int(np.max(y))\n",
    "    error = np.abs(np.array(prediction)-np.array(y))\n",
    "    average_error, ns = [0] * (maximal_price - minimal_price + 1), [0] * (maximal_price - minimal_price + 1)\n",
    "    \n",
    "    for err, p, act_price in zip(error, prediction, y):\n",
    "        idx = int(act_price) - minimal_price\n",
    "        average_error[idx] = average_error[idx] * ns[idx] / (ns[idx] + 1) + err / (ns[idx] + 1)\n",
    "        ns[idx] += 1\n",
    "        if idx > 0:\n",
    "            average_error[idx - 1] = average_error[idx - 1] * ns[idx - 1] / (ns[idx - 1] + 1) + err / (ns[idx - 1] + 1)\n",
    "            ns[idx - 1] += 1 \n",
    "        if idx < len(average_error) - 1:\n",
    "            average_error[idx + 1] = average_error[idx + 1] * ns[idx + 1] / (ns[idx + 1] + 1) + err / (ns[idx + 1] + 1)\n",
    "            ns[idx + 1] += 1\n",
    "            \n",
    "    std = [0] * (maximal_price - minimal_price + 1)\n",
    "    for err, act_price in zip(error, y):\n",
    "        idx = int(act_price) - minimal_price\n",
    "        std[idx] += (err - average_error[idx]) ** 2 / ns[idx]\n",
    "        if idx > 0:\n",
    "            std[idx-1] += (err - average_error[idx-1]) ** 2 / ns[idx-1]\n",
    "        if idx < len(average_error) - 1:\n",
    "            std[idx+1] += (err - average_error[idx+1]) ** 2 / ns[idx+1]\n",
    "            \n",
    "    std = np.sqrt(np.array(std))\n",
    "            \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Distribution of actual prices in test subset\")\n",
    "    plt.xlabel(\"Actual price [$]\")\n",
    "    plt.ylabel(\"Number of cab rides with fixed price\")\n",
    "    counts, bins = np.histogram(y, bins = (maximal_price - minimal_price) // 3)\n",
    "    plt.hist(bins[:-1], bins, weights=counts)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Distribution of predicted prices in test subset\")\n",
    "    plt.xlabel(\"Predicted price [$]\")\n",
    "    plt.ylabel(\"Number of cab rides with prediced price\")\n",
    "    counts, bins = np.histogram(prediction, bins = (maximal_price - minimal_price) // 3)\n",
    "    plt.hist(bins[:-1], bins, weights=counts)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.xlim((0, 100))\n",
    "    plt.title(\"Prediction error\")\n",
    "    plt.xlabel(\"Actual price [$]\")\n",
    "    plt.ylabel(\"Average error [$]\")\n",
    "    plt.grid()\n",
    "    plt.errorbar([i for i in range(minimal_price, maximal_price + 1)], average_error, np.sqrt(std), linestyle='None', marker='.')\n",
    "    plt.show()\n",
    "    \n",
    "def draw_result_3d(model, X, y, n=2, m=50):\n",
    "    \"\"\"Draw predictions and actual prices in 3d plot and compare them\"\"\"\n",
    "    if n==2:\n",
    "        pred = model.predict(X)\n",
    "    else:\n",
    "        X = PCA(n_components=2).fit_transform(X)\n",
    "    pred = pred.reshape(pred.shape[0])\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=X[0:m,0], y=X[0:m,1], z=y[0:m], mode='markers', name=\"Actual price\"))\n",
    "    fig.add_trace(go.Scatter3d(x=X[0:m,0], y=X[0:m,1], z=pred[0:m], mode='markers', name=\"Predicted price\"))\n",
    "    fig.show()\n",
    "    print(\"Prediction score square sum =\", predition_score(pred, y))\n",
    "    for i in range(m): print(\"actual price =\", \"{:5.2f}\".format(y[i]), \"predicted price =\", \\\n",
    "                        \"{:5.2f}\".format(abs(pred[i])))\n",
    "    \n",
    "def linear_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, m=50, \\\n",
    "                            do_print_ceof=False, do_draw_results=True, \\\n",
    "                            do_score=False, do_analyze_error = False):\n",
    "    \"\"\"Linear regression model - predict prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.coef_, model.intercept_)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.score(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "\n",
    "def gaussian_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                              m=50, do_print_ceof=False, do_draw_results=True, \\\n",
    "                              do_score=False, do_analyze_error = False):\n",
    "    \"\"\"Gaussian regression model - predict prices\"\"\"\n",
    "    \"\"\"Giving some a priori weights is probably necessary\"\"\"\n",
    "    \"\"\"and may have practical usage in our project - uber\"\"\"\n",
    "    \"\"\"may want to give some general prediction about prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    size = min(X_train.shape[0], size)\n",
    "    X_train, X_test, y_train, y_test = X_train[:size,:], X_test[:size // 4,:], y_train[:size], y_test[:size // 4]\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.L_, model.alpha_)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.score(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "        \n",
    "def nn_regression_model(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, m=50, \\\n",
    "                        do_print_ceof=False, do_draw_results=True, epochs=10, \\\n",
    "                        do_score=False, layer_n=1, do_analyze_error=False, \\\n",
    "                        do_analyze_epochs=False):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = prepare_data(sourcepath, pca_com=n)\n",
    "    layer_width = 8\n",
    "    if n: layer_width = int(np.sqrt(n))\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    if do_analyze_epochs:  \n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = epochs, verbose=1, callbacks=callbacks_list)\n",
    "    else:  \n",
    "        model.fit(X_train, y_train, epochs=epochs)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.evaluate(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dimensionality_lin_reg():\n",
    "    ns, scores = [65], []\n",
    "    best_n, best_score = 65, linear_regression_model(do_draw_results = False, do_score = True)\n",
    "    scores.append(best_score)\n",
    "    for n in range(64, 2, -2):\n",
    "        score = linear_regression_model(n = n, do_draw_results = False, do_score = True)\n",
    "        if score < best_score: best_n, best_score = n, score\n",
    "        ns.append(n)\n",
    "        scores.append(score)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.title(\"Linear regression score for different data dimensionality\")\n",
    "    plt.xlabel(\"n - data dimensionality\")\n",
    "    plt.ylabel(\"Linear regression score\")\n",
    "    plt.plot(ns, scores)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}\")\n",
    "    \n",
    "    return best_n\n",
    "    \n",
    "def find_best_nn():\n",
    "    ns, epochs, layers_ns, scores = [], [], [], []\n",
    "    best_score, best_n, best_epoch, best_layer_n = inf, 0, 0, 0\n",
    "    for n in [10, 30, 50, False]:\n",
    "        epoch = 10\n",
    "        for layer_n in range(3):            \n",
    "            print(f\"n = {n}, epoch = {epoch}, layer_n = {layer_n}\")\n",
    "            score = nn_regression_model(n = n, epochs = epoch, do_draw_results = False, \\\n",
    "                                        do_score = True, layer_n = layer_n)\n",
    "            if score < best_score: \n",
    "                best_n, best_score, best_epoch, best_layer_n = n, score, epoch, layer_n\n",
    "            ns.append(n)\n",
    "            layers_ns.append(layer_n)\n",
    "            scores.append(score)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=ns, y=layers_ns, z=scores, mode='markers', name=\"Prediction score\"))\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}, epoch = {best_epoch}, layer number = {best_layer_n + 1}\")\n",
    "    \n",
    "    return best_n, best_epoch, best_layer_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# best_n, best_epoch, best_layer_n = 50, 10, 1\n",
    "best_n, best_epoch, best_layer_n = find_best_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = 50\n",
    "best_layer_n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = best_epoch, layer_n = best_layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = best_epoch, layer_n = best_layer_n, do_draw_results=False, do_analyze_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score = nn_regression_model(n = best_n, epochs = 50, layer_n = best_layer_n, do_analyze_epochs = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cab rides analyze with exact condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dict(weather_dataframe):\n",
    "    loc_dict = dict()\n",
    "    for loc in np.unique(weather_dataframe['location']): \n",
    "        loc_dict[loc] = []\n",
    "    return loc_dict\n",
    "\n",
    "def fill_dict(loc_dict, weather_dataframe):\n",
    "    for row in weather_dataframe.to_numpy(): \n",
    "        loc_dict[row[1]].append(row)\n",
    "    return loc_dict\n",
    " \n",
    "def sort_cond(loc_dict):\n",
    "    for loc, rows in loc_dict.items(): \n",
    "        rows.sort(key=lambda x: x[5])\n",
    "    return loc_dict\n",
    "\n",
    "def find_condition(loc_dict, row, i):\n",
    "    t, conditions = row[2], loc_dict[row[i]]\n",
    "    a, b = 0, len(conditions) - 1\n",
    "    while a < b - 1:\n",
    "        c = (a + b) // 2\n",
    "        if conditions[c][5] < t: a = c\n",
    "        else: b = c\n",
    "    return conditions[c]    \n",
    "\n",
    "def append_rows(loc_dict, prices_dataframe):\n",
    "    data, data_merged = prices_dataframe.to_numpy(), []\n",
    "    for row in data:\n",
    "        row = np.append(row, find_condition(loc_dict, row, 4), 0)\n",
    "        data_merged.append(np.append(row, find_condition(loc_dict, row, 3), 0))\n",
    "    return data_merged\n",
    "\n",
    "def join_dataframes(path=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"Function to join data from two files: cab_rides.csv, weather.xls\"\"\"\n",
    "    filename_1, filename_2 = \"cab_rides.csv\", \"weather.xls\"\n",
    "    prices_dataframe = pd.read_csv(path + filename_1).dropna(axis=0).reset_index(drop=True)\n",
    "    prices_dataframe = prices_dataframe.drop('id', axis=1)\n",
    "    prices_dataframe['time_stamp'] = prices_dataframe['time_stamp'] // 1000\n",
    "    weather_dataframe = pd.read_csv(path + filename_2).fillna(0)\n",
    "    loc_dict = init_dict(weather_dataframe)\n",
    "    loc_dict = fill_dict(loc_dict, weather_dataframe)\n",
    "    loc_dict = sort_cond(loc_dict)\n",
    "    data_merged = append_rows(loc_dict, prices_dataframe)\n",
    "    df = pd.DataFrame(np.array(data_merged), columns = names_list)\n",
    "    df = df.drop('destination_location', axis=1).drop('source_location', axis=1)\n",
    "    df = df.drop('destination_time_stamp', axis=1).drop('source_time_stamp', axis=1)\n",
    "    return df\n",
    "\n",
    "def preprocess_data(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = join_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "def analyze_dataframe_joined(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = join_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    draw_heatmap(data, \"cab_rides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", n=False, \\\n",
    "                                      do_print_ceof=False, do_draw_results=True, \\\n",
    "                                      do_score = False, do_analyze_error=False):\n",
    "    \"\"\"Linear regression model - predict prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.coef_, model.intercept_)\n",
    "    print(\"Test score =\", model.score(X_test, y_test))\n",
    "    if do_draw_results: draw_result_3d(model, X_test, y_test)  \n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "\n",
    "def gaussian_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                                        do_print_ceof=False, do_draw_results=True, \\\n",
    "                                        do_score = False, do_analyze_error=False):\n",
    "    \"\"\"Gaussian regression model - predict prices\"\"\"\n",
    "    \"\"\"Giving some a priori weights is probably necessary\"\"\"\n",
    "    \"\"\"and may have practical usage in our project - uber\"\"\"\n",
    "    \"\"\"may want to give some general prediction about prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    size = min(X_train.shape[0], size)\n",
    "    X_train, X_test, y_train, y_test = X_train[:size,:], X_test[:size // 4,:], y_train[:size], y_test[:size // 4]\n",
    "    model = GaussianProcessRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    if do_print_ceof: print(\"Coefficients:\\n\", model.L_, model.alpha_)\n",
    "    print(\"Test score =\", model.score(X_test, y_test))\n",
    "    if do_draw_results: draw_result_3d(model, X_test, y_test)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)\n",
    "        \n",
    "def nn_regression_model_condition(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", size=20000, n=False, \\\n",
    "                                  do_print_ceof=False, do_draw_results=True, epochs=10, \\\n",
    "                                  m=50, do_score=False, layer_n=1, do_analyze_error=False, \\\n",
    "                                  do_analyze_epochs=False):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(sourcepath, pca_com=n)\n",
    "    X_train, X_test, y_train, y_test = X_train.astype('float32'), X_test.astype('float32'), y_train.astype('float32'), y_test.astype('float32')\n",
    "    layer_width = 8\n",
    "    if n: layer_width = int(np.sqrt(n))\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(layer_width, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    if do_analyze_epochs:  \n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = epochs, verbose=1, callbacks=callbacks_list)\n",
    "    else:  \n",
    "        model.fit(X_train, y_train, epochs=epochs)\n",
    "    if do_draw_results: \n",
    "        print(\"Test score =\", model.evaluate(X_test, y_test))\n",
    "        draw_result_3d(model, X_test, y_test, m=m)\n",
    "    if do_score: return score_model(model, X_test, y_test)\n",
    "    if do_analyze_error: error_analyze(model, X_test, y_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_dataframe_joined()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_dimensionality_lin_reg_condition():\n",
    "    ns, scores = [65], []\n",
    "    best_n, best_score = 65, linear_regression_model_condition(do_draw_results = False, do_score = True)\n",
    "    scores.append(best_score)\n",
    "    for n in range(64, 2, -2):\n",
    "        score = linear_regression_model_condition(n = n, do_draw_results = False, do_score = True)\n",
    "        if score < best_score: best_n, best_score = n, score\n",
    "        ns.append(n)\n",
    "        scores.append(score)\n",
    "    plt.title(\"Linear regression score for different data dimensionality\")\n",
    "    plt.xlabel(\"n - data dimensionality\")\n",
    "    plt.ylabel(\"Linear regression score\")\n",
    "    plt.plot(ns, scores)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}\")\n",
    "    \n",
    "    return best_n\n",
    "    \n",
    "def find_best_nn_condition():\n",
    "    ns, epochs, layers_ns, scores = [], [], [], []\n",
    "    best_score, best_n, best_epoch, best_layer_n = inf, 0, 0, 0\n",
    "    for n in [10, 30, 50, False]:\n",
    "        epoch = 10\n",
    "        for layer_n in range(3):            \n",
    "            print(f\"n = {n}, epoch = {epoch}, layer_n = {layer_n}\")\n",
    "            score = nn_regression_model_condition(n = n, epochs = epoch, do_draw_results = False, \\\n",
    "                                        do_score = True, layer_n = layer_n)\n",
    "            if score < best_score: \n",
    "                best_n, best_score, best_epoch, best_layer_n = n, score, epoch, layer_n\n",
    "            ns.append(n)\n",
    "            layers_ns.append(layer_n)\n",
    "            scores.append(score)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter3d(x=ns, y=layers_ns, z=scores, mode='markers', name=\"Prediction score\"))\n",
    "    fig.show()\n",
    "    \n",
    "    print(f\"Best score = {best_score} for n = {best_n}, epoch = {best_epoch}, layer number = {best_layer_n + 1}\")\n",
    "    \n",
    "    return best_n, best_epoch, best_layer_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# best_n, best_epoch, best_layer_n = 50, 10, 1\n",
    "best_n, best_epoch, best_layer_n = find_best_nn_condition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn_regression_model_condition(n = best_n, epochs = best_epoch, layer_n = best_layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nn_regression_model_condition(n = best_n, epochs = best_epoch, layer_n = best_layer_n, do_draw_results=False, do_analyze_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = nn_regression_model_condition(n = best_n, epochs = 50, layer_n = best_layer_n, do_draw_results=False, do_analyze_epochs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataframe_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\"):\n",
    "    \"\"\"PCA analyze for the whole dataset\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    return data.columns, data\n",
    "\n",
    "def split_dataframe_analyse(data):\n",
    "    \"\"\"Split dataframe into feature data and labels - price values\"\"\"\n",
    "    y = data['price']\n",
    "    X = data.drop('price', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=True)\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), y_train.to_numpy(), y_test.to_numpy()\n",
    "\n",
    "def prepare_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def prepare_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  \n",
    "\n",
    "def compare_scale(X_1, X_2):\n",
    "    r = [1] * 64\n",
    "    for i in range(64):\n",
    "        r[i] = (np.max(X_2[:,i]) - np.min(X_2[:,i])) / (np.max(X_1[:,i]) - np.min(X_1[:,i]))\n",
    "    r[2] = 1\n",
    "    return r\n",
    "\n",
    "def nn_regression_model(X_train, X_test, y_train, y_test, epochs=10, layer_n=1):\n",
    "    \"\"\"Neural network model predicting the prices\"\"\"\n",
    "    layer_width = 8\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for _ in range(layer_n): model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def average_features(X):\n",
    "    idx_av = [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    group1 = [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "    group2 = [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
    "    group3 = [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n",
    "    group4 = [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
    "    idx_moda = [1]\n",
    "    x = [0] * 64\n",
    "    for i in range(64): x[i] = np.min(X[:,i])\n",
    "    m1, m2 = [0] * 64, [0] * 64\n",
    "    for i in idx_av: x[i] = np.mean(X[:,i])\n",
    "    y = [0] * 64\n",
    "    for i in group1: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group2: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group3: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    y = [0] * 64\n",
    "    for i in group4: y[i] = np.sum(X[:,i])\n",
    "#     print(np.argmax(y))\n",
    "    x[np.argmax(y)] = np.max(X[:,np.argmax(y)])\n",
    "    for i in range(64):\n",
    "        m1[i] = np.min(X[:,i])\n",
    "        m2[i] = np.max(X[:,i])\n",
    "    for i in idx_moda:\n",
    "        values, counts = np.unique(X[i], return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        x[i] = values[ind]\n",
    "    return x, m1, m2\n",
    "    \n",
    "# model = nn_regression_model(X_train, X_test, y_train, y_test, epochs = 50, layer_n = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_analyse(\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "x_average, min_x, max_x = average_features(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_regression_model(X_train, X_test, y_train, y_test, epochs = 10, layer_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_coeff(model, x_average, min_x, max_x, i):\n",
    "    xs, ys = [], []\n",
    "    x_av = x_average.copy()\n",
    "    for j in range(11):\n",
    "        x = min_x[i] + (max_x[i] - min_x[i]) * j / 10\n",
    "        x_av[i] = x \n",
    "        xs.append(x)\n",
    "        ys.append(model.predict(np.array(x_av).reshape(1, -1)))\n",
    "    Reg = LinearRegression().fit(np.array(xs).reshape(-1, 1), np.array(ys).reshape(-1, 1))\n",
    "    return Reg.coef_[0][0]\n",
    "\n",
    "def find_coeff_2(model, x_average, min_x, max_x, i, a):\n",
    "    xs, ys = [], []\n",
    "    x_av = x_average.copy()\n",
    "    x_av[a] = min_x[a]\n",
    "    for j in range(2):\n",
    "        x = (1 - j) * min_x[i] + max_x[i] * j\n",
    "        x_av[i] = x \n",
    "        xs.append(x)\n",
    "        ys.append(model.predict(np.array(x_av).reshape(1, -1)))\n",
    "    return -((ys[1] - ys[0]) / (xs[1] - ys[0]))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0] * 64\n",
    "for i in [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n",
    "    a[i] = find_coeff(model, x_average, min_x, max_x, i)\n",
    "for i in [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 21)\n",
    "for i in [28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 29)\n",
    "for i in [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 41)\n",
    "for i in [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 52)\n",
    "for i in [1]:\n",
    "    a[i] = find_coeff_2(model, x_average, min_x, max_x, i, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "X_train_n, X_test_n, y_train_n, y_test_n = prepare_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "r = compare_scale(X_train_n, X_train)\n",
    "\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    print(f\"a_{i+1} = {a[i] / r[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_analyse(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "def preprocess_data_normal(sourcepath=\"..\\\\data\\\\cab_rides\\\\\", pca_com=False):\n",
    "    \"\"\"Preprocess data for prediction models\"\"\"\n",
    "    data = merge_dataframes(sourcepath)\n",
    "    data = refactor_data(data)\n",
    "    X_train, X_test, y_train, y_test = split_dataframe_analyse(data)\n",
    "#     X_train, X_test, y_train, y_test = split_dataframe(data)\n",
    "    if pca_com: X_train, X_test = down_scale_vector(X_train, X_test, pca_com)\n",
    "    return X_train, X_test, y_train, y_test  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data_analyse(\"..\\\\data\\\\cab_rides\\\\\", pca_com=False)\n",
    "x_average, min_x, max_x = average_features(X_train)\n",
    "print(x_average, min_x, max_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
